---
permalink: /
#title: "Qing Zhang's Personal Homepage"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<span style="color:rgb(10, 30, 183); font-size: 28px;font-weight: bold;">
About
</span>
<div style="height: 10px;"></div>  

<p style="text-align:justify;text-justify:inter-ideograph">
I received my Ph.D. degree at East China University of Science and Technology (ECUST) in 2012, supervised by Prof. Jiajun Lin. I received my B.S. degree at ECUST in 2007. My research interests include saliency detection, camouflaged object detection, multi-modal image segmentation, and computer vision.
</p>
<div style="height: 15px;"></div>  

<span style="color:rgba(10, 30, 183); font-size: 28px;font-weight: bold;">
Selected Papers
</span>
<div style="height: 10px;"></div>  

<span style="color:rgb(16, 28, 119); font-size: 22px;font-weight: bold;">2025</span>
<style>
  .colored-line {
    border: 0;
    height: 2px;
    background-color: rgb(201, 85, 184);
  }
</style>

<div class="colored-line"></div>   

<div style="height: 20px;"></div>

<div style="display:flex; margin-bottom: 20px; height:110px;">
<div style="flex:1;padding-right:10px">  

<img src="https://raw.githubusercontent.com/ZhangQing0329/ZhangQing0329.github.io/master/_pages/images/net.png" alt="network" style="max-width:100%; height:auto;">
</div>
<div style="flex:2.5; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">   
<strong>CGCOD: Class-Guided Camouflaged Object Detection</strong><br>
Chenxi Zhang, <strong>Qing Zhang*</strong>, Jiayun Wu, Youwei Pang*<br> 
ACM International Conference on Multimedia (<em>ACM MM</em>), 2025<br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-A</strong></span>] [<a href="https://github.com/bbdjj/CGCOD" target="_blank">Code</a>]      
</div>
</div>

<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:10px"> 
<img src="https://raw.githubusercontent.com/ZhangQing0329/ZhangQing0329.github.io/master/_pages/images/icme2025.png" alt="network-zhao" style="max-width:100%; height:auto;"> 
</div>
<div style="flex:2.5; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">  
<strong>Frequency-guided Camouflaged Object Detection with Perceptual Enhancement and Dynamic Balance</strong><br>
Yuetong Li, Yilin Zhao, <strong>Qing Zhang*</strong>, Qiangqiang Zhou, Yanjiao Shi<br>
IEEE International Conference on Multimedia & Expo (<em>ICME</em>), 2025<br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-B</strong></span>] [<a href="https://github.com/iuueong/FPDNet" target="_blank">Code</a>]     
</div>
</div> 

<div style="display:flex; margin-bottom: 20px; height:160px;">
<div style="flex:1;padding-right:10px"> 
<img src="https://raw.githubusercontent.com/ZhangQing0329/ZhangQing0329.github.io/master/_pages/images/icassp-zhang.png" alt="network" style="max-width:100%; height:auto;">
</div>
<div style="flex:2.5; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">     
<strong>Rethinking Camouflaged Object Detection via Foreground-background Interactive Learning</strong><br>    
Chenxi Zhang, <strong>Qing Zhang*</strong>, Jiayun Wu<br>    
International Conference on Acoustics, Speech, and Signal Processing (<em>ICASSP</em>), 2025<br>  
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-B</strong>
</span>] [<a href="https://github.com/bbdjj/FBINet" target="_blank">Code</a>] [<a href="https://ieeexplore.ieee.org/document/10889016" target="_blank">PDF</a>]    
</div>
</div>    
      
<div style="display:flex; margin-bottom: 20px; height:150px;">
<div style="flex:1;padding-right:10px"> 
<img src="https://raw.githubusercontent.com/ZhangQing0329/ZhangQing0329.github.io/master/_pages/images/BDCL.png" alt="network2" style="max-width:100%; height:auto;">
</div>
<div style="flex:2.5; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">     
<strong>Bilateral Decoupling Complementarity Learning Network for Camouflaged Object Detection</strong><br>   
Rui Zhao, Yuetong Li, <strong>Qing Zhang*</strong>, Xinyi Zhao<br>     
Knowledge-Based Systems (<em>KBS</em>), 2025, 314: 113158<br>    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院1区, Top</strong>
</span>] [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705125002059" target="_blank">PDF</a>]    
</div>
</div>    
         
<div style="display:flex; margin-bottom: 20px; height:160px;">
<div style="flex:1;padding-right:10px"> 
<img src="https://raw.githubusercontent.com/ZhangQing0329/ZhangQing0329.github.io/master/_pages/images/icassp-zhao2025.png" alt="network3" style="max-width:100%; height:auto;">
</div>
<div style="flex:2.5; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">     
<strong>Camouflaged Object Detection with CNN-Transformer Harmonization and Calibration</strong><br>   
Yilin Zhao, <strong>Qing Zhang*</strong>, Yuetong Li<br>
International Conference on Acoustics, Speech, and Signal Processing (<em>ICASSP</em>), 2025<br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-B</strong>
</span>] [<a href="https://ieeexplore.ieee.org/document/10887964" target="_blank">PDF</a>]    
</div>
</div>    

<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)"> 
<strong>FLRNet: A Bio-inspired Three-stage Network for Camouflaged Object Detection via Filtering, Localization and Refinement</strong><br>
Yilin Zhao, <strong>Qing Zhang*</strong>, Yuetong Li<br> 
Neurocomputing (<em>NEURO</em>), 2025, 626: 129523<br> 
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院2区</strong></span>] [<a href="https://www.sciencedirect.com/science/article/abs/pii/S092523122500195X" target="_blank">PDF</a>]   
</div>
</div>    

<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)"> 
<strong>HRPVT: High-Resolution Pyramid Vision Transformer for Medium and Small-scale Human Pose Estimation</strong><br>
Zhoujie Xu, Meng Dai*, <strong>Qing Zhang</strong>, Xiaodi Jiang<br>
Neurocomputing (<em>NEURO</em>), 2025, 619(28): 129154<br> 
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院2区</strong></span>] [<a href="https://https://www.sciencedirect.com/science/article/pii/S0925231224019258" target="_blank">PDF</a>]   
</div>
</div>  

<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)"> 
<strong>A Dual-stream Learning Framework for Weakly Supervised Salient Object Detection with Multi-strategy Integration</strong><br>    
Yuyan Liu, <strong>Qing Zhang*</strong>, Yilin Zhao, Yanjiao Shi<br>
The Visual Computer (<em>TVC</em>), 2025<br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院3区</strong>
</span>] [<a href="https://github.com/boom118/BSnet" target="_blank">Code</a>] </span>] [<a href="https://link.springer.com/article/10.1007/s00371-025-03798-9" target="_blank">PDF</a>]   
</div>
</div>     


<div style="display:flex; margin-bottom: 20px; height:110px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)"> 
<strong>FGNet: Feature Calibration and Guidance Refinement for Camouflaged Object Detection</strong><br>      
Qiang Yu, <strong>Qing Zhang*</strong>, Yanjiao Shi, Qiangqiang Zhou<br>
International Joint Conference on Neural Networks (<em>IJCNN</em>), 2025<br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong>
</span>] [<a href="https://github.com/ZhangQing0329/FGNet" target="_blank">Code</a>]    
</div>
</div>      

<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">      
<strong>HEFNet: Hierarchical Unimodal Enhancement and Multi-modal Fusion for RGB-T Salient Object Detection</strong><br>     
Jiayun Wu, <strong>Qing Zhang*</strong>, Chenxi Zhang, Yanjiao Shi, Qiangqiang Zhou<br>      
International Joint Conference on Neural Networks (<em>IJCNN</em>), 2025<br>   
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong>
</span>] [<a href="https://github.com/ZhangQing0329/HEFNet" target="_blank">Code</a>]    
</div>
</div>      

 <div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Dual-domain Collaboration Learning Network for Camouflaged Object Detection</strong><br>    
Jingming Wang, <strong>Qing Zhang*</strong>, Xinyi Zhao, Yanjiao Shi, Qiangqiang Zhou<br>      
International Joint Conference on Neural Networks (<em>IJCNN</em>), 2025<br>   
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong>
</span>] [<a href="https://github.com/ZhangQing0329/DCLNet" target="_blank">Code</a>]    
</div>
</div>     

<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Direction-Oriented Edge Reconstruction Network for Camouflaged Object Detection</strong><br>    
Xiaoxu Yang, <strong>Qing Zhang*</strong>, Qiangqiang Zhou<br>    
International Joint Conference on Neural Networks (<em>IJCNN</em>), 2025<br>    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong>
</span>] [<a href="https://github.com/splendid131313/DOERNet" target="_blank">Code</a>]    
</div>
</div> 

<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Boundary-and-Object Collaborative Learning Network for Camouflaged Object Detection</strong><br>    
Chenyu Zhuang, <strong>Qing Zhang*</strong>, Chenxi Zhang, Xinxin Yuan<br>    
Image and Vision Computing (<em>IMAVIS</em>), 2025, 161: 105596<br>    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院3区</strong>
</span>] [<a href="https://github.com/ZhangQing0329/BCLNet" target="_blank">Code</a>] [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885625001842" target="_blank">PDF</a>]   
</div>
</div>    

<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>FINet: Detecting Camouflaged Objects Using Frequency-aware Information</strong><br>      
Chengfeng Zhu, Zeming Liu, <strong>Qing Zhang*</strong><br>    
The Chinese Conference on Pattern Recognition and Computer Vision (<em>PRCV</em>), 2025<br>   
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong></span>]  
</div>
</div>     

<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>SAM2-LPNet: Saliency Guided and Laplacian Aware Fine-tuning of SAM2 for Weakly Supervised Salient Object Detection</strong> <br>      
The Chinese Conference on Pattern Recognition and Computer Vision (<em>PRCV</em>), 2025 <br> 
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
<strong>CCF-C</strong>
</span>] 
</div>
</div>    

<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Collaborative Perception and Dual-stage Decoder Network for Camouflaged Object Detection</strong><br>       
The Chinese Conference on Pattern Recognition and Computer Vision (<em>PRCV</em>), 2025 <br> 
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
<strong>CCF-C</strong>
</span>] 
</div>
</div>   

<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Intra- and Inter-Group Mutual Learning Network for Lightweight Camouflaged Object Detection</strong><br>    
Chenyu Zhuang, <strong>Qing Zhang*</strong><br>
International Conference on Intelligent Computing (<em>ICIC</em>), 2025: 164-176 <br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong>
</span>] [<a href="https://link.springer.com/chapter/10.1007/978-981-96-9961-2_14" target="_blank">PDF</a>]   
</div>
</div>   

<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Rethinking Lightweight and Efficient Human Pose Esrimation with Star Operation Reconstruction</strong><br>  
Xu Zhoujie, Meng Dai*, <strong>Qing Zhang</strong>, Huawen Liu <br>
International Conference on Knowledge Science, Engineering and Management (<em>KSEM</em>), 2025 <br>  
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong>
</span>] 
</div>
</div>      

<div style="display:flex; margin-bottom: 20px; height:150px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>A Camouflaged Object Dection Network with Global Cross-space Perception and Flexible Local Feature Refinement Network</strong><br>
Zhenjie Ji, Yanjiao Shi*, <strong>Qing Zhang</strong>, Qiangqiang Zhou<br>
International Conference on Artificial Neural Networks (<em>ICANN</em>), 2025<br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong>
</span>] 
</div>
</div>    


<span style="color:rgb(16, 28, 119); font-size: 22px;font-weight: bold;">2024</span>
<style>
  .colored-line {
    border: 0;
    height: 2px;
    background-color: rgb(201, 85, 184);
  }
</style>

<div class="colored-line"></div>   

<div style="height: 20px;"></div>  

<div style="display:flex; margin-bottom: 20px; height:160px;">
<div style="flex:1;padding-right:10px"> 
<img src="https://raw.githubusercontent.com/ZhangQing0329/ZhangQing0329.github.io/master/_pages/images/TCSVT-zhang2024.png" alt="network4" style="max-width:100%; height:auto;">
</div>
<div style="flex:2.5; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">     
<strong>Salient Object Detection with Edge-guided Learning and Specific Aggregation</strong> <br> 
Liqian Zhang, <strong>Qing Zhang*</strong> <br> 
IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>), 2024, 34(1): 534-548<br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院1区, Top</strong>
</span>] [<a href="https://github.com/ZhangQing0329/ELSANet" target="_blank">Code</a>] [<a href="https://ieeexplore.ieee.org/document/10155248" target="_blank">PDF</a>]   
</div>
</div>         
      
<div style="display:flex; margin-bottom: 20px; height:160px;">
<div style="flex:1;padding-right:10px"> 
<img src="https://raw.githubusercontent.com/ZhangQing0329/ZhangQing0329.github.io/master/_pages/images/icme2024.png" alt="network5" style="max-width:100%; height:auto;">
</div>
<div style="flex:2.5; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">     
<strong>Bi-directional Boundary-object Interaction and Refinement Network for Camouflaged Object Detection</strong><br>  
Jicheng Yang, <strong>Qing Zhang*</strong>, Yilin Zhao, Yuetong Li, Zeming Liu<br>
IEEE International Conference on Multimedia and Expo (<em>ICME</em>), 2024<br> 
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-B</strong>
</span>] [<a href="https://github.com/Jcogito/BIRNet" target="_blank">Code</a>] [<a href="https://ieeexplore.ieee.org/document/10687766" target="_blank">PDF</a>]   
</div>
</div>     
       
<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Detecting Camouflaged Object via Cross-level Context Supplement</strong><br>
<strong>Qing Zhang*</strong>, Weiqi Yan, Rui Zhao, Yanjiao Shi<br>
Applied Intelligence, 2024 <br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院3区</strong>
</span>] [<a href="https://github.com/ZhangQing0329/CCSNet" target="_blank">Code</a>] [<a href="https://link.springer.com/article/10.1007/s10489-024-05694-6" target="_blank">PDF</a>]   
</div>
</div>       
      
<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>BMFNet: Bifurcated Multi-Modal Fusion Network for RGB-D Salient Object Detection</strong><br>  
Chenwang Sun, <strong>Qing Zhang*</strong>, Chenyu Zhuang, Mingqian Zhang<br>
Image and Vision Computing (<em>IMAVIS</em>), 2024, 147: 105048<br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院3区</strong>
</span>] [<a href="https://github.com/ZhangQing0329/BMFNet" target="_blank">Code</a>] [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0262885624001525" target="_blank">PDF</a>]   
</div>
</div>     
      
<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Frequency Learning Network with Dual-Guidance Calibration for Camouflaged Object Detection</strong><br> 
Yilin Zhao, <strong>Qing Zhang*</strong>, Yuetong Li<br>
Asian Conference on Computer Vision (<em>ACCV</em>), 2024. <br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong>
</span>] [<a href="https://github.com/LinAyq/DCNet" target="_blank">Code</a>] [<a href="https://openaccess.thecvf.com/content/ACCV2024/papers/Zhao_Frequency_Learning_Network_with_Dual-Guidance_Calibration_for_Camouflaged_Object_Detection_ACCV_2024_paper.pdf" target="_blank">PDF</a>]   
</div>
</div>   
      
<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Transformer-based Depth Optimization Network for RGB-D Salient Object Detection</strong><br>   
Lu Li, Yanjiao Shi*, Jingyu Yang, Qiangqiang Zhou, <strong>Qing Zhang</strong>, Liu Cui<br>  
International Conference on Pattern Recognition (<em>ICPR</em>), 2024. <br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>CCF-C</strong>
</span>] [<a href="https://link.springer.com/chapter/10.1007/978-3-031-78305-0_28" target="_blank">PDF</a>]   
</div>
</div>  
      
<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>AFINet: Camouflaged object detection via attention fusion and interaction network</strong><br>  
<strong>Qing Zhang</strong>, Weiqi Yan, Yilin Zhao, Qi Jin, Yu Zhang*<br>  
Journal of Visual Communication and Image Representation (<em>JVCIR</em>), 2024 <br> 
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院4区</strong>
</span>] [<a href="https://github.com/ZhangQing0329/AFINet" target="_blank">Code</a>] [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1047320324001640" target="_blank">PDF</a>]   
</div>
</div>    
      
<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Boosting transferability of adversarial samples via saliency distribution and frequency domain enhancement</strong> <br> 
Yixuan Wang, Wei Hong, Xueqin Zhang*, <strong>Qing Zhang</strong>, Chunhua Gu <br> 
Knowledge-Based Systems (<em>KBS</em>), 2024, 300: 112152 <br> 
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院1区, Top</strong>
</span>] [<a href="https://www.sciencedirect.com/science/article/abs/pii/S095070512400786X" target="_blank">PDF</a>]   
</div>
</div> 

<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Light-sensitive and adaptive fusion network for RGB-T crowd counting</strong><br>
Liangjun Huang*, Wencan Kang, Guangkai Chen, <strong>Qing Zhang</strong>, Jianwei Zhang <br>
The Visual Computer (<em>TVC</em>), 2024, 40(10): 7279-7292 <br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院3区</strong>
</span>] [<a href="https://link.springer.com/article/10.1007/s00371-024-03388-1" target="_blank">PDF</a>]   
</div>
</div>   

<div style="display:flex; margin-bottom: 20px; height:120px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>Multi-branch feature fusion and refinement network for salient object detection</strong><br>
Jinyu Yang, Yanjiao Shi*, Jin Zhang, Qianqian Guo, <strong>Qing Zhang</strong>, Liu Cui<br>
Multimedia Systems, 2024, 30(4): 190  <br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院4区</strong>
</span>] [<a href="https://link.springer.com/article/10.1007/s00530-024-01356-2" target="_blank">PDF</a>]   
</div>
</div>   
  
<div style="display:flex; margin-bottom: 20px; height:140px;">
<div style="flex:1;padding-right:1px"> 
</div>
<div style="flex:500; padding-left:10px; border-left:4px solid rgba(139, 28, 136, 1)">       
<strong>D2Net: discriminative feature extraction and details preservation network for salient object detection</strong><br>
Qianqian Guo, Yanjiao Shi*, Jin Zhang, Jinyu Yang, <strong>Qing Zhang</strong><br>
Journal of Electronic Imaging (<em>JEI</em>), 2024, 33(4): : 043047 <br>
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;"><strong>中科院4区</strong>
</span>] [<a href="https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-33/issue-4/043047/D2Net--discriminative-feature-extraction-and-details-preservation-network-for/10.1117/1.JEI.33.4.043047.short" target="_blank">PDF</a>]   
</div>
</div>     
        
<span style="color:rgb(16, 28, 119); font-size: 22px;font-weight: bold;">2023</span>
<style>
  .colored-line {
    border: 0;
    height: 2px;
    background-color: rgb(201, 85, 184);
  }
</style>

<div class="colored-line"></div>   

<div style="height: 20px;"></div>       
      
**CFANet: A Cross-layer Feature Aggregation Network for Camouflaged Object Detection**  
**Qing Zhang\***, Weiqi Yan  
IEEE International Conference on Multimedia and Expo (***ICME***), 2023: 2441-2446    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**CCF-B**
</span>]  
[\[PDF\]](https://ieeexplore.ieee.org/document/10219858) [\[Code\]](https://github.com/ZhangQing0329/CFANet)
      
**TCRNet: a trifurcated cascaded refinement network for salient object detection**  
**Qing Zhang\***, Rui Zhao, Liqian Zhang  
IEEE Transactions on Circuits and Systems for Video Technology (***TCSVT***), 2023, 33(1): 298-311    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院1区, Top**
</span>]  
[\[PDF\]](https://ieeexplore.ieee.org/document/9861608/)  [\[Code\]](https://github.com/ZhangQing0329/TCRNet)   
           
**Polyp-Mixer: An Efficient Context-Aware MLP-based Paradigm for Polyp Segmentation**  
Jinghui Shi, **Qing Zhang\***, Yuhao Tang, Zhongqun Zhang  
IEEE Transactions on Circuits and Systems for Video Technology (***TCSVT***), 2023, 33(1): 30-42   
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院1区, Top**
</span>]  
[\[PDF\]](https://ieeexplore.ieee.org/document/9852486/)  

**Depth cue enhancement and guidance network for RGB-D salient object detection**  
Xiang Li, **Qing Zhang\***, Weiqi Yan, Meng Dai  
Journal of Visual Communication and Image Representation (***JVCIR***), 2023, 95: 103880    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院4区**
</span>]  
[\[PDF\]](https://authors.elsevier.com/sd/article/S1047-3203(23)00130-X)  [\[Code\]](https://github.com/ZhangQing0329/DEGNet)  

**Motion Analysis and Reconstruction of Human Joint Regions for Sparse RGBD Images**   
Tianzhen Dong, Yuntao Bai, **Qing Zhang**, Yi Zhang   
IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (***VRW***), 2023: 773-774   
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**CCF-A**
</span>]     
[\[PDF\]](https://ieeexplore.ieee.org/document/10108739/)    
     
**SC2Net: Scale-aware Crowd Counting Network with Pyramid Dilated Convolution**
Lanjun Liang\*, Huailin Zhao, Fangbo Zhou, **Qing Zhang**, Zhili Song, Qingxuan Shi    
Applied Intelligence, 2023, 53(5): 5146-5159   
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院3区**
</span>]    

<span style="color:rgb(16, 28, 119); font-size: 22px;font-weight: bold;">2022</span>
<style>
  .colored-line {
    border: 0;
    height: 2px;
    background-color: rgb(201, 85, 184);
  }
</style>

<div class="colored-line"></div>   

<div style="height: 20px;"></div> 

**Progressive dual-attention residual network for salient object detection**  
Liqian Zhang, **Qing Zhang\***, Rui Zhao  
IEEE Transactions on Circuits and Systems for Video Technology (***TCSVT***), 2022, 32(9): 5902-5915   
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院1区, Top**
</span>]   
[\[PDF\]](https://ieeexplore.ieee.org/document/9745960/) [\[Code\]](https://github.com/ZhangQing0329/PDRNet)    
      
**Residual attentive feature learning network for salient object detection**  
**Qing Zhang\***, Yanjiao Shi, Xueqin Zhang, Liqian Zhang  
Neurocomputing (***NEURO***), 2022, 51:741-752    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院2区**
</span>]  
[\[PDF\]](https://www.sciencedirect.com/science/article/abs/pii/S0925231222007846)    
      
**Cross-modal and multi-level feature refinement network for RGB-D salient object detection**  
Yue Gao, Meng Dai\*, **Qing Zhang**  
The Visual Computer (***TVC***), 2022, 39: 3979-3994      
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院3区**
</span>]  
[\[PDF\]](https://link.springer.com/article/10.1007/s00371-022-02543-w)    

**COMAL: compositional multi-scale feature enhanced learning for crowd counting**     
Fangbo Zhou, Huailin Zhao\*, Yani Zhang, **Qing Zhang**, Lanjun Liang, Yaoyao Li, Zuodong Duan     
Multimedia Tools and Applications, 2022, 81(15): 20541-20560    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院4区**
</span>]   
[\[PDF\]](https://link.springer.com/article/10.1007/s11042-022-12249-9)    

**R2Net: Residual refinement network for salient object detection**     
Jin Zhang, Qiuwei Liang, Qianqian Guo, Jinyu Yang, **Qing Zhang**, Yanjiao Shi\*    
Image and Vision Computing (***IMAVIS***), 2022, 120: 104423   
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院3区**
</span>]   
[\[PDF\]](https://www.sciencedirect.com/science/article/abs/pii/S026288562200052X)    
    
**Attention guided contextual feature fusion network for salient object detection**    
Jin Zhang, Yanjiao Shi\*, **Qing Zhang**, Liu Cui, Ying Chen, Yugen Yi   
Image and Vision Computing (***IMAVIS***), 2022, 117: 104337    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院3区**
</span>]  
[\[PDF\]](https://www.sciencedirect.com/science/article/abs/pii/S0262885621002420)          

<span style="color:rgb(16, 28, 119); font-size: 22px;font-weight: bold;">2021</span>
<style>
  .colored-line {
    border: 0;
    height: 2px;
    background-color: rgb(201, 85, 184);
  }
</style>

<div class="colored-line"></div>   

<div style="height: 20px;"></div>  

**Global and local information aggregation network for edge-aware salient object detection**  
**Qing Zhang\***, Liqian Zhang, Dong Wang, Yanjiao Shi, Jianjun Lin  
Journal of Visual Communication and Image Representation (***JVCIR***), 2021, 81: 103350    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院4区**
</span>]  
[\[PDF\]](https://www.sciencedirect.com/science/article/abs/pii/S1047320321002303)          
       
**Deep saliency detection via spatial-wise dilated convolutional attention**  
Wenzhao Cui, **Qing Zhang\***, Baochuan Zuo  
Neurocomputing, 2021, 445: 35-49     
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院2区**
</span>]  
[\[PDF\]](https://www.sciencedirect.com/science/article/abs/pii/S0925231221003179)        
      
**Edge-aware salient object detection network via context guidance**  
Xiaowei Chen, **Qing Zhang\***, Liqian Zhang  
Image and Vision Computing (***IMAVIS***), 2021, 110: 104166    
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院3区**
</span>]  
[\[PDF\]](https://www.sciencedirect.com/science/article/abs/pii/S0262885621000718)     
     
**Salient object detection network with multi-scale feature  refinement and boundary feedback**   
**Qing Zhang\***, Xiang Li   
Image and Vision Computing (***IMAVIS***), 2021, 116: 104326      
[<span style="color:rgba(144, 55, 73, 1); font-weight: bold;">
**中科院3区**
</span>]  
[\[PDF\]](https://www.sciencedirect.com/science/article/abs/pii/S0262885621002316)     

<span style="color:rgb(16, 28, 119); font-size: 22px;font-weight: bold;">2020</span>
<style>
  .colored-line {
    border: 0;
    height: 2px;
    background-color: rgb(201, 85, 184);
  }
</style>

<div class="colored-line"></div>   

<div style="height: 20px;"></div>    
  
**Attention and boundary guided salient object detection**   
**Qing Zhang\***, Yanjiao Shi, Xueqin Zhang  
Pattern Recognition (***PR***), 2020, 107: 107-484    
[\[PDF\]](https://www.sciencedirect.com/science/article/abs/pii/S0031320320302879)     
      
**Attentive feature integration network for detecting salient objects in images**    
**Qing Zhang\***, Wenzhao Cui, Yanjiao Shi, Xueqin Zhang    
Neurocomputing, 2020, 411: 268-281    
[\[PDF\]](https://www.sciencedirect.com/science/article/abs/pii/S0925231220309346)     

<span style="color:rgba(16, 28, 119, 1); font-size: 22px;font-weight: bold;">2019 and Before</span>
<style>
  .colored-line {
    border: 0;
    height: 2px;
    background-color: rgb(63, 114, 175);
  }
</style>

<div class="colored-line"></div>   

<div style="height: 20px;"></div>    
   
**Multi-level and multi-scale deep saliency network for salient object detection**  
**Qing Zhang\***, Jiajun Lin, Jingjing Zhuge, Wenhao Yuan    
Journal of Visual Communication and Image Representation (***JVCIR***), 2019, 59: 415-424  
      
**Kernel null-space-based abnormal event detection using hybrid motion information**  
Yanjiao Shi, Yugen Yi, **Qing Zhang\***, Jiangyan Dai   
Journal of Electronic Imaging (***JEI***), 2019, 28(2): 021011  

**Hierarchical Salient Object Detection Network with Dense Connections**   
**Qing Zhang\***, Jianchen Shi, Baochuan Zuo, Meng Dai, Tianzhen Dong, Xiao Qi   
International Conference on Image and Graphics (***ICIG***), 2019: 454-466     
      
**Salient object detection via compactness and objectness cues**   
**Qing Zhang\***, Jiajun Lin, Wenju Li, Yanjiao Shi, Guogang Cao  
The Visual Computer (***TVC***), 2018, 34(4): 473-489  
      
**Salient object detection via color and texture cues**  
**Qing Zhang\***, Jiajun Lin, Yanyun Tao, Wenju Li, Yanjiao Shi    
Neurocomputing (***NEURO***), 2017, 3(21): 35-48  
      
**Two-stage absorbing markov chain for salient object detection**   
**Qing Zhang\***, Desi Luo, Wenju Li, Yanjiao Shi, Jiajun Lin    
International Conference on Image Processing (***ICIP***), 2017: 895-899   

**A systematic EHW approach to the evolutionary design of sequential circuits**   
Yanyun Tao\*, **Qing Zhang**, Lijun Zhang, Yuzhen Zhang    
Soft Computing, 2013, 20(12): 5025-5038        

**Salient Object Detection via Structure Extraction and Region Contrast**    
**Qing Zhang\***, Jiajun Lin, Xiaodan Li    
Journal of Information Science and Engineering (***JISE***),2016, 32(6): 1435-1454    

**Saliency-based abnormal event detection in crowded scenes**   
Yanjiao Shi\*, Yunxiang Liu, **Qing Zhang**, Yugen Yi, Wenju Li      
Journal of Electronic Imaging (***JEI***), 2016, 25(6): 61608   

**Structure extraction and region contrast based salient object detection**    
**Qing Zhang\***, Jiajun Lin, Zhigang Xie    
International Conference on Digital Image Processing (***ICDIP***), 2016.   

**Exemplar-based image inpainting using color distribution analysis**   
**Qing Zhang\***, Jiajun Lin    
Journal of Information Science and Engineering (***JISE***), 2012, 28(4): 641-654.    

**Image inpainting via variation of variances and linear weighted filling-in**   
**Qing Zhang\***, Jiajun Lin    
Optical Engineering, 2011, 50(7): 077001.

<div style="height: 40px;"></div> 

<a href="https://info.flagcounter.com/Qcxw"><img src="https://s01.flagcounter.com/map/Qcxw/size_s/txt_320EAB/border_F0F0F0/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a>


<div style="display:none">
A data-driven personal website
======
Like many other Jekyll-based GitHub Pages templates, Academic Pages makes you separate the website's content from its form. The content & metadata of your website are in structured markdown files, while various other files constitute the theme, specifying how to transform that content & metadata into HTML pages. You keep these various markdown (.md), YAML (.yml), HTML, and CSS files in a public GitHub repository. Each time you commit and push an update to the repository, the [GitHub pages](https://pages.github.com/) service creates static HTML pages based on these files, which are hosted on GitHub's servers free of charge.

Many of the features of dynamic content management systems (like Wordpress) can be achieved in this fashion, using a fraction of the computational resources and with far less vulnerability to hacking and DDoSing. You can also modify the theme to your heart's content without touching the content of your site. If you get to a point where you've broken something in Jekyll/HTML/CSS beyond repair, your markdown files describing your talks, publications, etc. are safe. You can rollback the changes or even delete the repository and start over - just be sure to save the markdown files! Finally, you can also write scripts that process the structured data on the site, such as [this one](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.ipynb) that analyzes metadata in pages about talks to display [a map of every location you've given a talk](https://academicpages.github.io/talkmap.html).


Getting started
======
1. Register a GitHub account if you don't have one and confirm your e-mail (required!)
2. Fork [this template](https://github.com/academicpages/academicpages.github.io) by clicking the "Use this template" button in the top right. 
3. Go to the repository's settings (rightmost item in the tabs that start with "Code", should be below "Unwatch"). Rename the repository "[your GitHub username].github.io", which will also be your website's URL.
4. Set site-wide configuration and create content & metadata (see below -- also see [this set of diffs](http://archive.is/3TPas) showing what files were changed to set up [an example site](https://getorg-testacct.github.io) for a user with the username "getorg-testacct")
5. Upload any files (like PDFs, .zip files, etc.) to the files/ directory. They will appear at https://[your GitHub username].github.io/files/example.pdf.  
6. Check status by going to the repository settings, in the "GitHub pages" section

Site-wide configuration
------
The main configuration file for the site is in the base directory in [_config.yml](https://github.com/academicpages/academicpages.github.io/blob/master/_config.yml), which defines the content in the sidebars and other site-wide features. You will need to replace the default variables with ones about yourself and your site's github repository. The configuration file for the top menu is in [_data/navigation.yml](https://github.com/academicpages/academicpages.github.io/blob/master/_data/navigation.yml). For example, if you don't have a portfolio or blog posts, you can remove those items from that navigation.yml file to remove them from the header. 

Create content & metadata
------
For site content, there is one markdown file for each type of content, which are stored in directories like _publications, _talks, _posts, _teaching, or _pages. For example, each talk is a markdown file in the [_talks directory](https://github.com/academicpages/academicpages.github.io/tree/master/_talks). At the top of each markdown file is structured data in YAML about the talk, which the theme will parse to do lots of cool stuff. The same structured data about a talk is used to generate the list of talks on the [Talks page](https://academicpages.github.io/talks), each [individual page](https://academicpages.github.io/talks/2012-03-01-talk-1) for specific talks, the talks section for the [CV page](https://academicpages.github.io/cv), and the [map of places you've given a talk](https://academicpages.github.io/talkmap.html) (if you run this [python file](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.py) or [Jupyter notebook](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.ipynb), which creates the HTML for the map based on the contents of the _talks directory).

**Markdown generator**

The repository includes [a set of Jupyter notebooks](https://github.com/academicpages/academicpages.github.io/tree/master/markdown_generator
) that converts a CSV containing structured data about talks or presentations into individual markdown files that will be properly formatted for the Academic Pages template. The sample CSVs in that directory are the ones I used to create my own personal website at stuartgeiger.com. My usual workflow is that I keep a spreadsheet of my publications and talks, then run the code in these notebooks to generate the markdown files, then commit and push them to the GitHub repository.

How to edit your site's GitHub repository
------
Many people use a git client to create files on their local computer and then push them to GitHub's servers. If you are not familiar with git, you can directly edit these configuration and markdown files directly in the github.com interface. Navigate to a file (like [this one](https://github.com/academicpages/academicpages.github.io/blob/master/_talks/2012-03-01-talk-1.md) and click the pencil icon in the top right of the content preview (to the right of the "Raw | Blame | History" buttons). You can delete a file by clicking the trashcan icon to the right of the pencil icon. You can also create new files or upload files by navigating to a directory and clicking the "Create new file" or "Upload files" buttons. 

Example: editing a markdown file for a talk
![Editing a markdown file for a talk](/images/editing-talk.png)

For more info
------
More info about configuring Academic Pages can be found in [the guide](https://academicpages.github.io/markdown/), the [growing wiki](https://github.com/academicpages/academicpages.github.io/wiki), and you can always [ask a question on GitHub](https://github.com/academicpages/academicpages.github.io/discussions). The [guides for the Minimal Mistakes theme](https://mmistakes.github.io/minimal-mistakes/docs/configuration/) (which this theme was forked from) might also be helpful.
</div>